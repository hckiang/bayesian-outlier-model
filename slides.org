#+TITLE:     Outlier Analysis with Bayesian inference
#+AUTHOR:    Hao Chi Kiang
#+EMAIL:     hckiang@riseup.net
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME:
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation]
#+LaTeX_CLASS_OPTIONS: [bigger]
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage[style=authoryear,backend=biber]{biblatex}
#+LATEX_HEADER: \bibliography{ref}
#+LATEX_HEADER: \DeclareMathOperator{\sinvchisq}{Scaled-inv-\chi^2}

* Statistical model for outlier analysis
** Simple univariate normal location-scale model
*** Normal location-scale model for outlier detection \autocite{verdnelli1990}
Let $y = (y_i)_{i\in I}$ be a family of random quantities with density
\[
p(y_i|\mu, \sigma^2, \epsilon, A_i) = (1 - \epsilon) \phi(y_i;\mu,\sigma^2) + \epsilon \phi(y_i;\mu + A_i,\sigma^2)
\]
where

    - Assume probability of outlying $\epsilon$ is known and $\epsilon \in\, ]0,0.5[$
    - $\phi$ is the normal density.
    - A = (A_i)_{i\in I} are 'outliers' displacement'

Note that weighted sum of any probability measures is still probability measure.

*** Re-expression of location-scale problem
- For each $i \in I$, let $\delta_i$ be independent Bernouli variable, with
  $E(\delta_i) = \epsilon$. Then
  \[
  y_i | \mu, \sigma^2, A, \delta \sim N(\mu + \delta_iA_i, \sigma^2)
  \]

  + $\delta_i = 1$ means $y_i$ is an outlier.\\
  + Assume $(A_i)_{i\in I}$ are mutually independent

- We need 4 conditional distributions for Gibbs sampling

*** Conditional posteriors for Gibbs sampling
- Let $y^*_i = y_i - \delta_i A_i$, then $y^*_i \sim N(\mu, \sigma^2)$
- Semi-conjugate prior for $\mu$ and $\sigma$
\[
\mu |A, \delta \sim N(\mu_0, \tau_0^2)
\]
\[
\sigma^2 | A, \delta ~ \sim \sinvchisq(\nu_0, \sigma_0^2)
\]

- Posterior
  \begin{align*}
    \mu |y, \sigma^2, A, \delta &\sim N\left(\frac{\tau_0^2\mu_0 + \sigma^{-2}\sum_{i\in I} y^*_i}{\tau_0^2 + n\sigma^{-2}}, \frac{1}{(\tau_0^2 + n\sigma^{-2})}\right)\\
    \sigma^2 | \mu, y, A, \delta &\sim \sinvchisq(\nu_0 + n, \frac{\nu_0\sigma_0^2 + \sum_{i\in I}({y^*_{i}} - \mu)^2}{\nu_0 + n})
  \end{align*}
  where
    \begin{align*}
     n &= |I|
    \end{align*}

*** Conditional posteriors for Gibbs sampling
- The Bernouli outlier assignments $\delta_i$:
  \[
    E(\delta_i|y, \mu, \sigma^2, A) = \frac{\epsilon \phi(y_i; \mu+A_i, \sigma^2)}{\epsilon \phi(y_i; \mu+A_i, \sigma^2) + (1 - \epsilon) \phi(y_i; \mu, \sigma^2)}
  \]
- The outlier displacement $A_i$
  - Prior $A_i \sim N(0, \xi^{-2})$
  - Posterior
    \begin{align*}
      p(A_i|y,\mu,\sigma^2,\delta) =& \delta_i\phi\left(A_i;y_i - \mu, (\xi^2 + \sigma^{-2})^{-1}\right)\\
      &  + (1 - \delta_i)\phi(A_i;0, \xi^{-2})
    \end{align*}
- Now we have all the posterior conditional distributions. Gibbs sampling!
*** Alternative: assuming unknown $\epsilon$
- In practice, setting $\epsilon = 0.05$ works well
- But we can treat it as random variable if we want
  + Assume $\epsilon$ depends only on $\delta$
  + Prior
    \[
      \epsilon \sim Beta(\alpha, \beta)
    \]
  + Posterior
    \[
      \epsilon|\delta = Beta\left(\alpha + \sum_{i\in I}\delta_i, \beta + \sum_{i\in I}(1 - \delta_i)\right)
    \]
*** How to set $\alpha$ and $\beta$ properly?
  + User set the $E(\epsilon)$ for the prior
  + Additionally, we assum $P(\epsilon < \frac{1}{2}) = 0.99$
  + Solve equations r.w.t $\alpha$ and $\beta$
    \begin{equation*}
    \begin{cases}
         E(\epsilon) = \frac{\alpha}{\alpha + \beta}\\
         \frac{\int_0^{\frac{1}{2}} t^{\alpha - 1} (1 - t)^{\beta - 1} dt}{\int_0^1 t^{\alpha - 1} (1 - t)^{\beta - 1} dt} = 0.99
    \end{cases}
    \end{equation*}

    - Can be done in R with `uniroot()` and `pbeta()` with three lines of codes

*** 
#+ATTR_LaTeX: :width .9\textwidth
[[./onedim.pdf]]
** Mixture of normals with outlier detection
*** Mixture of normals with outlier detection
- Multiple mixtures of normals N(\mu_k, \sigma_k^2), $k\in K$
- Mixture Weights: $(w_k)_{k\in K}$
- Augment data with
  + Mixture indicator $(z_i)_{i\in I}$ where $z_i \in \{1,...,k\}$
  + Outliers indicator $(\delta_i)_{i\in I}$
  + Outliers' displacement $(A_i)_{i\in I}$
- Likelihood becomes
  \[
    p(y_i | z,\mu,\sigma,\delta,A,w) = \sum_{k\in K} w_k\phi(y_i;\mu_k+\delta_iA_i, \sigma_k^2) (z_i = k)
  \]
  where $(E)$ denotes indicator function of random event $E$
- *Note that $A_i$ depends on $z_i$ now*
*** Mixture of normals with outlier detection
- Let $y^*_i = y_i - \delta_i A_i$. Then
  \[
    p(y^*_i | z,\mu,\sigma,\delta,A,w) = \sum_{k\in K} w_k\phi(y^*_i;\mu_k+\delta_iA_i, \sigma_k^2) (z_i = k)
  \]
- Semi-conjugate prior for the $\mu$'s and $\sigma$'s
  \begin{align*}
    \mu_k |A, \delta, w,z_i,w,\epsilon &\sim N(\mu_0, \tau_0^2)\\
    \sigma_k^2 | A, \delta, w,z_i,\epsilon &\sim \sinvchisq(\nu_0, \sigma_0^2) \\
  \end{align*}

*** Mixture of normals with outlier detection
- Conditional Posterior for the $\mu$'s and $\sigma$'s
  \footnotesize
  \begin{align*}
    \mu |y, \sigma^2, A, \delta, w, z_i,\epsilon &\sim N\left(\frac{\tau_0^2\mu_0 + \sigma^{-2}\sum_{i\in I_k} y^*_i}{\tau_0^2 + n_k\sigma^{-2}}, \frac{1}{(\tau_0^2 + n_k\sigma^{-2})}\right)\\
    \sigma^2 | y,\mu, A, \delta, w, z_i,\epsilon &\sim \sinvchisq\left(\nu_0 + n_k, \frac{\nu_0\sigma_0^2 + \sum_{i\in I_k}({y^*_{i}} - \mu)^2}{\nu_0 + n_k}\right)
  \end{align*}
  \normalsize
  where
    \begin{align*}
     I_k &= \{i \in I: z_i = k\}\\
     n_k &= |I_k|\\
    \end{align*}
*** Mixture of normals with outlier detection
- Conditional posterior for $z$
  \small
  \[
    p(z_i|y,w,\mu, \sigma, w, A, \delta, \epsilon) = \sum_{k\in K} \frac{w_k\phi(y_i;\mu_k,\sigma_k^2)}{\displaystyle\sum_{m\in K}w_m\phi(y_i;\mu_m,\sigma_m^2)}(z_i = k)
  \]
  \normalsize
- Conditional posterior for $w$
  - Depends only on $z$
  - Use the Dirichlet-Multinomial conjugate prior
    \[
      w | y, \mu, \sigma, A, \delta, \epsilon ~ Dirichlet(\gamma)
    \]
  - Then posterior is
    \small
    \[
      w | y, \mu, \sigma, z, A, \delta, \epsilon ~ Dirichlet\left( (n_k + \gamma_k)_{k\in K} \right)
    \]
    \normalsize

*** Mixture of normals with outlier detection
- Conditional posterior for $\delta_i$
  \small
  \[
    E(\delta_i|y, \mu, \sigma^2, w, z, A, \epsilon) = \frac{\epsilon \phi(y_i; \dot{\mu}_i+A_i, \dot{\sigma}_i^2)}{\epsilon \phi(y_i; \dot{\mu}_i+A_i, \dot{\sigma}_i^2) + (1 - \epsilon) \phi(y_i; \dot{\mu}_i, \dot{\sigma}_i^2)}
  \]
  where
  \begin{align*}
    \dot{\mu}_i &= \sum_{k \in K} \mu_k(z_i = k)\\
    \dot{\sigma}_i^2 &= \sum_{k \in K} \sigma_k^2(z_i = k)\\
  \end{align*}
  \normalsize

*** Mixture of normals with outlier detection
- Conditional posterior for $A_i$
  - Prior $A_i \sim N(0, \xi^{-2})$
    \begin{align*}
      p(A_i|y,\mu,\sigma^2,w,z,\delta,\epsilon) =& \delta_i\phi\left(A_i;y_i - \dot{\mu}_i, (\xi^2 + \dot{\sigma}_i^{-2})^{-1}\right)\\
      &  + (1 - \delta_i)\phi(A_i;0, \xi^{-2})
    \end{align*}
  - Completely the same as non-mixture version, except $\mu$ and $\sigma$ becomes $\dot{\mu}_i$ and $\dot{\sigma}_i$
- Considiton posterior of $\epsilon$ is the same as non-mixture version

*** 
#+ATTR_LATEX: :width 0.8\linewidth
[[./rainfall.pdf]]
*** 
#+ATTR_LATEX: :width 0.8\linewidth
[[./manymixes.pdf]]

** Mixture of multivariate normals with outlier detection
*** Mixture of multivariate normals with outlier detection
- Everything is the same for multivariate, except $\mu$ and $\Sigma$
- Use Normal-Inverse-Wishart joint prior instead

    \fontsize{0.80em}{}
    \begin{align*}
    &\Sigma_k | y, A, \delta, w, z_i,\epsilon \sim IW\left(\nu_0 + n_k, \sigma_0 + S_k + \frac{k_0n_k}{k_0 + n_k}(m_k - \mu_0)(m_k - \mu_0)^T\right)\\
    &\mu_k |y, \Sigma, A, \delta, w, z_i,\epsilon \sim N\left(\frac{n_k m_k + \kappa_0 \mu_0}{n_k + \kappa_0}, \frac{1}{\kappa_0}\Sigma_k \right)\\
    \end{align*}
    \small
    where
      \begin{align*}
      m_k &= \frac{1}{n_k}\sum_{i \in I_k} y^*_i\\
      S_k &= \sum_{i \in I_k} (y^*_i - m_k)(y^*_i - m_k)^T\\
      \end{align*}
    \normalsize

*** 
#+ATTR_LATEX: :width 0.8\linewidth
[[./2dmix.pdf]]
** Bayesian Regression with outlier detection
*** Bayesian Regression with outlier detection
- Simple linear regression model
   \[
     y_i | \beta = \beta^Tx_i + r_i
   \]
  where $(r_i)_{i\in I}$ are mutually independent $N(0, \sigma^2)$
- But it hurts when there are outlying residuals
- Solution: Augment the data and redefine the likelihood
   \[
     y_i | \beta, \sigma^2, A, \epsilon, (\delta_i = 1) = \beta^T x_i + r_i
   \]
   \[
     y_i | \beta, \sigma^2, A, \epsilon, (\delta_i = 0) = A_i + r_i
   \]
   - Implication:
     + $r_i = 0 + \delta_i A_i$
     + \beta doesn't depend on outlying data any more!
*** Bayesian Regression with outlier detection
- Joint prior for $\beta$ and $\sigma$
  \begin{align*}
      \beta |\sigma^2, \delta, A, \epsilon &\sim N(\nu_0, \sigma^2\Omega_0^{-1})\\
      \sigma^2 |\delta, A, \epsilon &\sim \sinvchisq(\nu_0, \sigma_0^2)
  \end{align*}
- Posterior
  \fontsize{0.80em}{}
  \begin{align*}
        &\beta |y, \sigma^2, \delta, A, \epsilon \sim N\left(\Omega_n^{-1}(\dot{X}^T \dot{y} + \Omega_0\mu_0), \sigma^2(\dot{X}^T \dot{X} + \Omega_0)^{-1}\right)\\
        &\sigma^2 |y, \delta, A, \epsilon \sim \sinvchisq\left(\nu_n, \frac{1}{\nu_n}(\nu_0\sigma_0^2 + (\dot{y}^T\dot{y} + \mu_0^T\Omega_0\mu_0 - \mu_n^T\Omega_n\mu_n))\right)
  \end{align*}
  where
  \begin{align*}
  \dot{X} &= \left[x_{j_1}\;x_{j_2}\;\cdots\right]^T,\quad j_n \in \{i: \delta_i = 1\} \forall n\\
  \dot{Y} &= \left[y_{j_1}\;y_{j_2}\;\cdots\right]^T\\
  \nu_n &= \nu_0 + \sum_{i \in I}\delta_i\\
  \Omega_n &= \dot{X}^T \dot{X} + \Omega_0
  \end{align*}
  \normalsize

*** Bayesian Regression with outlier detection
- Conditional posterior of $\delta$
  \small
  \[
    E(\delta_i|y, \beta, \sigma^2, A, \epsilon) = \frac{\epsilon \phi(r_i; A_i, \dot{\sigma}_i^2)}{\epsilon \phi(r_i; A_i, \dot{\sigma}_i^2) + (1 - \epsilon) \phi(r_i; 0, \dot{\sigma}_i^2)}
  \]
  - Note that $r$ is given when both $y$, $\beta$ are given
- Conditonal posterior of $A$ and $\epsilon$ are just same as before

*** 
#+ATTR_LATEX: :width 0.8\linewidth
[[./fueleff.pdf]]

*** 
**** Works Cited
\printbibliography
**** Source code
[[https://github.com/hckiang/bayesian-outlier-model][https://github.com/hckiang/bayesian-outlier-model]]
**** $\quad$                                               :BMCOL:B_block:
:PROPERTIES:
:BEAMER_col: 0.5
:BEAMER_env: block
:BEAMER_envargs: <2->
:END:
#+ATTR_LATEX: :width .4\linewidth
[[./github_qrcode.png]]
